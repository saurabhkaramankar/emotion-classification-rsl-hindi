{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "#installing tensorflow since it has a set of libraries and tools that support the building process of machine learning models."
      ],
      "metadata": {
        "id": "mnSb82CceCWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc73b6a-0cda-46ea-82a8-fb94cf805110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Embedding, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Bhaav-Dataset.csv', encoding='utf-8')\n",
        "\n",
        "# Preprocess the dataset\n",
        "X = df['Sentences'].values\n",
        "y = df['Annotation'].values\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X)\n",
        "word_index = tokenizer.word_index\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_length = 100\n",
        "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_classes = len(set(y))\n",
        "y = tf.keras.utils.to_categorical(y, num_classes)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 32, input_length=max_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax')) # there is dropout in this model and\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "vo3nsUS5-jbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d827de-2c12-456a-e63f-ef25e95c1018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "508/508 [==============================] - 6s 10ms/step - loss: 1.2686 - accuracy: 0.5746 - val_loss: 1.2345 - val_accuracy: 0.5799\n",
            "Epoch 2/20\n",
            "508/508 [==============================] - 6s 11ms/step - loss: 1.1820 - accuracy: 0.5786 - val_loss: 1.1947 - val_accuracy: 0.5843\n",
            "Epoch 3/20\n",
            "508/508 [==============================] - 5s 9ms/step - loss: 1.0284 - accuracy: 0.6099 - val_loss: 1.2117 - val_accuracy: 0.5718\n",
            "Epoch 4/20\n",
            "508/508 [==============================] - 5s 11ms/step - loss: 0.8532 - accuracy: 0.6742 - val_loss: 1.3212 - val_accuracy: 0.5696\n",
            "Epoch 5/20\n",
            "508/508 [==============================] - 5s 10ms/step - loss: 0.6870 - accuracy: 0.7428 - val_loss: 1.4557 - val_accuracy: 0.5454\n",
            "Epoch 6/20\n",
            "508/508 [==============================] - 5s 9ms/step - loss: 0.5370 - accuracy: 0.8080 - val_loss: 1.6339 - val_accuracy: 0.5235\n",
            "Epoch 7/20\n",
            "508/508 [==============================] - 6s 11ms/step - loss: 0.4287 - accuracy: 0.8530 - val_loss: 1.9820 - val_accuracy: 0.5331\n",
            "Epoch 8/20\n",
            "508/508 [==============================] - 5s 9ms/step - loss: 0.3374 - accuracy: 0.8904 - val_loss: 2.1700 - val_accuracy: 0.5151\n",
            "Epoch 9/20\n",
            "508/508 [==============================] - 5s 10ms/step - loss: 0.2630 - accuracy: 0.9157 - val_loss: 2.3551 - val_accuracy: 0.4972\n",
            "Epoch 10/20\n",
            "508/508 [==============================] - 5s 10ms/step - loss: 0.2177 - accuracy: 0.9346 - val_loss: 2.6393 - val_accuracy: 0.5016\n",
            "Epoch 11/20\n",
            "508/508 [==============================] - 5s 9ms/step - loss: 0.1736 - accuracy: 0.9505 - val_loss: 2.8990 - val_accuracy: 0.4996\n",
            "Epoch 12/20\n",
            "508/508 [==============================] - 6s 11ms/step - loss: 0.1442 - accuracy: 0.9586 - val_loss: 3.0542 - val_accuracy: 0.4858\n",
            "Epoch 13/20\n",
            "508/508 [==============================] - 5s 9ms/step - loss: 0.1180 - accuracy: 0.9676 - val_loss: 3.4993 - val_accuracy: 0.5107\n",
            "Epoch 14/20\n",
            "508/508 [==============================] - 5s 10ms/step - loss: 0.1011 - accuracy: 0.9719 - val_loss: 3.9455 - val_accuracy: 0.5218\n",
            "Epoch 15/20\n",
            "508/508 [==============================] - 5s 11ms/step - loss: 0.0960 - accuracy: 0.9749 - val_loss: 3.7967 - val_accuracy: 0.5006\n",
            "Epoch 16/20\n",
            "508/508 [==============================] - 5s 9ms/step - loss: 0.0798 - accuracy: 0.9795 - val_loss: 4.2383 - val_accuracy: 0.5119\n",
            "Epoch 17/20\n",
            "508/508 [==============================] - 6s 11ms/step - loss: 0.0756 - accuracy: 0.9806 - val_loss: 4.3835 - val_accuracy: 0.5159\n",
            "Epoch 18/20\n",
            "508/508 [==============================] - 5s 9ms/step - loss: 0.0734 - accuracy: 0.9797 - val_loss: 4.4362 - val_accuracy: 0.5004\n",
            "Epoch 19/20\n",
            "508/508 [==============================] - 5s 10ms/step - loss: 0.0669 - accuracy: 0.9825 - val_loss: 4.5453 - val_accuracy: 0.4935\n",
            "Epoch 20/20\n",
            "508/508 [==============================] - 6s 12ms/step - loss: 0.0553 - accuracy: 0.9861 - val_loss: 4.6703 - val_accuracy: 0.4945\n",
            "127/127 [==============================] - 0s 3ms/step - loss: 4.6703 - accuracy: 0.4945\n",
            "[4.670283317565918, 0.49445948004722595]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# Load the dataset from the .csv file\n",
        "data = pd.read_csv('Bhaav-Dataset.csv')\n",
        "\n",
        "# Split the dataset into input sentences and corresponding labels\n",
        "sentences = data['Sentences'].values\n",
        "labels = data['Annotation'].values\n",
        "\n",
        "# Perform label encoding on the emotion labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text and convert sentences to sequences\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "# Pad the sequences to have a consistent length\n",
        "max_seq_length = max([len(seq) for seq in train_sequences + test_sequences])\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=100, input_length=max_seq_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax')) #model architecture lacks dropout\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_data, test_labels)\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXqnmpnFqOG7",
        "outputId": "7fd81b7c-09c2-4113-b30f-d42b89b5a1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "508/508 [==============================] - 35s 67ms/step - loss: 1.2144 - accuracy: 0.5757 - val_loss: 1.1620 - val_accuracy: 0.5907\n",
            "Epoch 2/10\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.9351 - accuracy: 0.6478 - val_loss: 1.1175 - val_accuracy: 0.5838\n",
            "Epoch 3/10\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.5277 - accuracy: 0.8117 - val_loss: 1.3620 - val_accuracy: 0.5629\n",
            "Epoch 4/10\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.2167 - accuracy: 0.9354 - val_loss: 1.7345 - val_accuracy: 0.5211\n",
            "Epoch 5/10\n",
            "508/508 [==============================] - 35s 69ms/step - loss: 0.0958 - accuracy: 0.9772 - val_loss: 2.0087 - val_accuracy: 0.5454\n",
            "Epoch 6/10\n",
            "508/508 [==============================] - 34s 66ms/step - loss: 0.0621 - accuracy: 0.9873 - val_loss: 2.1464 - val_accuracy: 0.5484\n",
            "Epoch 7/10\n",
            "508/508 [==============================] - 33s 65ms/step - loss: 0.0428 - accuracy: 0.9919 - val_loss: 2.5510 - val_accuracy: 0.5681\n",
            "Epoch 8/10\n",
            "508/508 [==============================] - 34s 66ms/step - loss: 0.0372 - accuracy: 0.9935 - val_loss: 2.2829 - val_accuracy: 0.5046\n",
            "Epoch 9/10\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.0314 - accuracy: 0.9946 - val_loss: 2.4460 - val_accuracy: 0.5319\n",
            "Epoch 10/10\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 0.0305 - accuracy: 0.9948 - val_loss: 2.3246 - val_accuracy: 0.5304\n",
            "127/127 [==============================] - 2s 12ms/step - loss: 2.3246 - accuracy: 0.5304\n",
            "Test Loss: 2.3246\n",
            "Test Accuracy: 0.5304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate predictions for the test set\n",
        "y_pred = model.predict(test_sentences)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded labels back to categorical labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute classification report\n",
        "report = classification_report(y_true, y_pred)\n",
        "\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "NG7xeQIc4OZ6",
        "outputId": "799bd584-3174-44e5-aec3-8e153da733a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-49f278c834d3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Generate predictions for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_2' (type Sequential).\n    \n    Input 0 of layer \"conv1d_2\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 100)\n    \n    Call arguments received by layer 'sequential_2' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=False\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Approach 2"
      ],
      "metadata": {
        "id": "AybRjCTR7aro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n"
      ],
      "metadata": {
        "id": "MW0OPaxi7aKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Bhaav-Dataset.csv\")\n"
      ],
      "metadata": {
        "id": "9B_c8jCU7k7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into input (text) and target (emotion) columns\n",
        "X = df[\"Sentences\"].values\n",
        "y = df[\"Annotation\"].values\n",
        "\n",
        "# Perform tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad sequences to ensure equal length\n",
        "max_length = max([len(x) for x in X])\n",
        "X = pad_sequences(X, maxlen=max_length)\n",
        "\n",
        "# Convert emotion labels to one-hot vectors\n",
        "label_map = {label: idx for idx, label in enumerate(np.unique(y))}\n",
        "y = np.array([label_map[label] for label in y])\n",
        "num_classes = len(label_map)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes)\n"
      ],
      "metadata": {
        "id": "jOr_IfsA7zlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset into train and test \n",
        "train_ratio = 0.8\n",
        "split_idx = int(train_ratio * len(X))\n",
        "\n",
        "X_train = X[:split_idx]\n",
        "y_train = y[:split_idx]\n",
        "X_test = X[split_idx:]\n",
        "y_test = y[split_idx:]\n"
      ],
      "metadata": {
        "id": "FEvGaScF8B7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "filters = 128\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "trPdkFYl8Ncr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('emotion_detection_model.h5')"
      ],
      "metadata": {
        "id": "YPEq15EQ89pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "FAm_HBpPCniP",
        "outputId": "73c30d19-adfc-4135-edc1-b6de88071293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 126, 100)          1806200   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 124, 128)          38528     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,845,373\n",
            "Trainable params: 1,845,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktiniKyW8WFs",
        "outputId": "64f6004e-dab8-4f8f-8227-cb677dcf83d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "508/508 [==============================] - 29s 56ms/step - loss: 1.2573 - accuracy: 0.5521 - val_loss: 1.0366 - val_accuracy: 0.6540\n",
            "Epoch 2/10\n",
            "508/508 [==============================] - 28s 56ms/step - loss: 0.9651 - accuracy: 0.6286 - val_loss: 1.0887 - val_accuracy: 0.6136\n",
            "Epoch 3/10\n",
            "508/508 [==============================] - 28s 55ms/step - loss: 0.5868 - accuracy: 0.7892 - val_loss: 1.2799 - val_accuracy: 0.5462\n",
            "Epoch 4/10\n",
            "508/508 [==============================] - 29s 57ms/step - loss: 0.3121 - accuracy: 0.8974 - val_loss: 1.4874 - val_accuracy: 0.5476\n",
            "Epoch 5/10\n",
            "508/508 [==============================] - 27s 54ms/step - loss: 0.1633 - accuracy: 0.9518 - val_loss: 1.7290 - val_accuracy: 0.5358\n",
            "Epoch 6/10\n",
            "508/508 [==============================] - 27s 54ms/step - loss: 0.0876 - accuracy: 0.9769 - val_loss: 1.9245 - val_accuracy: 0.5380\n",
            "Epoch 7/10\n",
            "508/508 [==============================] - 28s 55ms/step - loss: 0.0519 - accuracy: 0.9887 - val_loss: 2.1798 - val_accuracy: 0.5031\n",
            "Epoch 8/10\n",
            "508/508 [==============================] - 27s 54ms/step - loss: 0.0330 - accuracy: 0.9931 - val_loss: 2.3653 - val_accuracy: 0.5031\n",
            "Epoch 9/10\n",
            "508/508 [==============================] - 28s 55ms/step - loss: 0.0252 - accuracy: 0.9950 - val_loss: 2.4448 - val_accuracy: 0.5260\n",
            "Epoch 10/10\n",
            "508/508 [==============================] - 28s 55ms/step - loss: 0.0197 - accuracy: 0.9957 - val_loss: 2.5434 - val_accuracy: 0.5329\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f987aacc9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwPgj0tA85_F",
        "outputId": "eed07f7c-16ed-4da1-8e40-26dc62471850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127/127 [==============================] - 1s 9ms/step - loss: 2.5434 - accuracy: 0.5329\n",
            "Test Loss: 2.5433542728424072\n",
            "Test Accuracy: 0.5328736901283264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(\"emotion_detection_model.h5\")\n"
      ],
      "metadata": {
        "id": "NvtVmAKx84rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate predictions for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded labels back to categorical labels\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute classification report\n",
        "report = classification_report(y_true, y_pred, target_names=emotion_categories)\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "Fve1djrNivfu",
        "outputId": "de07cb3a-1127-484f-ab11-647893af4a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127/127 [==============================] - 1s 9ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e97c148b4b25>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Compute classification report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memotion_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'emotion_categories' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "eZaQGG0oipy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(sentence):\n",
        "    # Tokenize the sentence\n",
        "    sentence = tokenizer.texts_to_sequences([sentence])\n",
        "    sentence = pad_sequences(sentence, maxlen=max_length)\n",
        "\n",
        "    # Predict the emotion\n",
        "    emotion_probabilities = model.predict(sentence)[0]\n",
        "    predicted_emotion = np.argmax(emotion_probabilities)\n",
        "    \n",
        "    # Map the predicted label to the corresponding emotion\n",
        "    label_map_reverse = {v: k for k, v in label_map.items()}\n",
        "    predicted_emotion = label_map_reverse[predicted_emotion]\n",
        "    \n",
        "    return predicted_emotion\n"
      ],
      "metadata": {
        "id": "P6Cu5MKS9tZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"यह एक उदाहरण है\"\n",
        "predicted_emotion = predict_emotion(sentence)\n",
        "print(\"Predicted Emotion:\", predicted_emotion)\n"
      ],
      "metadata": {
        "id": "k4ZOYE6q9zSB",
        "outputId": "a25b226b-1885-4917-ba4e-8162bf63e83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c5844fc88e82>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"यह एक उदाहरण है\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Emotion:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_emotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_emotion' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 3"
      ],
      "metadata": {
        "id": "E9RNH26YTxsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# Load the Bhaav dataset\n",
        "data = pd.read_csv('Bhaav-Dataset.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "sentences = data['Sentences'].values\n",
        "labels = data['Annotation'].values\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_seq_length = 100\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Define the CNN model\n",
        "embedding_dim = 100\n",
        "num_filters = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_seq_length))\n",
        "model.add(Conv1D(num_filters, 3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "model.fit(train_data, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_data, test_labels))\n",
        "\n",
        "# Perform emotion prediction\n",
        "text = \"सुना है यहाँ मुर्दो की खोपडिया दौड़ती है \"\n",
        "input_sequence = tokenizer.texts_to_sequences([text])\n",
        "input_data = pad_sequences(input_sequence, maxlen=max_seq_length)\n",
        "predictions = model.predict(input_data)\n",
        "emotion_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "print(\"Input Text:\", text)\n",
        "print(\"Predicted Emotion:\", emotion_labels[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDOaMsjyTxIC",
        "outputId": "a0c7e44f-fbcf-4869-f285-9074365bf11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "254/254 [==============================] - 19s 70ms/step - loss: 1.2409 - accuracy: 0.5732 - val_loss: 1.1878 - val_accuracy: 0.5797\n",
            "Epoch 2/10\n",
            "254/254 [==============================] - 17s 67ms/step - loss: 1.0056 - accuracy: 0.6217 - val_loss: 1.1388 - val_accuracy: 0.6011\n",
            "Epoch 3/10\n",
            "254/254 [==============================] - 16s 64ms/step - loss: 0.5648 - accuracy: 0.7999 - val_loss: 1.3202 - val_accuracy: 0.5597\n",
            "Epoch 4/10\n",
            "254/254 [==============================] - 17s 69ms/step - loss: 0.2208 - accuracy: 0.9349 - val_loss: 1.7032 - val_accuracy: 0.5755\n",
            "Epoch 5/10\n",
            "254/254 [==============================] - 16s 64ms/step - loss: 0.0915 - accuracy: 0.9764 - val_loss: 1.9725 - val_accuracy: 0.5718\n",
            "Epoch 6/10\n",
            "254/254 [==============================] - 18s 73ms/step - loss: 0.0481 - accuracy: 0.9900 - val_loss: 2.1874 - val_accuracy: 0.5681\n",
            "Epoch 7/10\n",
            "254/254 [==============================] - 18s 72ms/step - loss: 0.0324 - accuracy: 0.9940 - val_loss: 2.2146 - val_accuracy: 0.5371\n",
            "Epoch 8/10\n",
            "254/254 [==============================] - 16s 64ms/step - loss: 0.0250 - accuracy: 0.9958 - val_loss: 2.3845 - val_accuracy: 0.5587\n",
            "Epoch 9/10\n",
            "254/254 [==============================] - 16s 63ms/step - loss: 0.0241 - accuracy: 0.9958 - val_loss: 2.3899 - val_accuracy: 0.5568\n",
            "Epoch 10/10\n",
            "254/254 [==============================] - 17s 69ms/step - loss: 0.0221 - accuracy: 0.9962 - val_loss: 2.4300 - val_accuracy: 0.5420\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "Input Text: सुना है यहाँ मुर्दो की खोपडिया दौड़ती है \n",
            "Predicted Emotion: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 4"
      ],
      "metadata": {
        "id": "MT2CleeIVLDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('Bhaav-Dataset.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "texts = data['Sentences'].values\n",
        "labels = data[['Annotation']].values\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels.argmax(axis=1))\n",
        "\n",
        "# Define the CNN model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, encoded_labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Predict the emotions for all sentences\n",
        "predicted_labels = model.predict_classes(padded_sequences)\n",
        "\n",
        "# Convert the predicted labels back to emotion labels\n",
        "predicted_emotions = label_encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "# Save the predicted emotions to a .txt file\n",
        "with open('predicted_emotions.txt', 'w') as file:\n",
        "    for label in predicted_emotions:\n",
        "        file.write(label + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "WKfHefCqakJ_",
        "outputId": "7674c819-b747-4d19-98b2-2e2ceaf84263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "508/508 [==============================] - 35s 68ms/step - loss: 0.0376 - accuracy: 0.9956 - val_loss: 5.6645e-05 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "508/508 [==============================] - 33s 64ms/step - loss: 1.1296e-05 - accuracy: 1.0000 - val_loss: 4.4248e-05 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "508/508 [==============================] - 36s 71ms/step - loss: 5.3265e-06 - accuracy: 1.0000 - val_loss: 3.8597e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "508/508 [==============================] - 36s 71ms/step - loss: 2.9719e-06 - accuracy: 1.0000 - val_loss: 3.5433e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "508/508 [==============================] - 34s 68ms/step - loss: 1.7564e-06 - accuracy: 1.0000 - val_loss: 3.3189e-05 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "508/508 [==============================] - 38s 75ms/step - loss: 1.0644e-06 - accuracy: 1.0000 - val_loss: 3.1645e-05 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "508/508 [==============================] - 36s 71ms/step - loss: 7.3390e-07 - accuracy: 1.0000 - val_loss: 3.0409e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "508/508 [==============================] - 33s 66ms/step - loss: 4.8134e-07 - accuracy: 1.0000 - val_loss: 2.9412e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "508/508 [==============================] - 34s 67ms/step - loss: 3.6735e-07 - accuracy: 1.0000 - val_loss: 2.8545e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "508/508 [==============================] - 33s 65ms/step - loss: 2.8774e-07 - accuracy: 1.0000 - val_loss: 2.7624e-05 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9bb8d1b11206>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Predict the emotions for all sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Convert the predicted labels back to emotion labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('Bhaav-Dataset.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "X = dataset['Sentences']\n",
        "y = dataset['Annotation']\n",
        "\n",
        "# Map numerical labels to emotions\n",
        "emotion_labels = {0: 'angry', 1: 'joy', 2: 'sad', 3: 'surprise', 4: 'neutral'}\n",
        "y = y.map(emotion_labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = max(len(sequence) for sequence in X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define the CNN model\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(emotion_labels), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predict the emotions for the test set\n",
        "predicted_labels = model.predict_classes(X_test)\n",
        "predicted_emotions = [emotion_labels[label] for label in predicted_labels]\n",
        "\n",
        "# Save the predicted emotions in a text file\n",
        "with open('predicted_emotions.txt', 'w') as file:\n",
        "    for emotion in predicted_emotions:\n",
        "        file.write(emotion + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "6J6Zqesvb5FA",
        "outputId": "a6077d9c-4af5-4dfc-9aeb-108a0fa1b299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cd3a9e4f7519>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Predict the emotions for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Approach 5\n"
      ],
      "metadata": {
        "id": "TkV0OedPfCGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import read_excel\n",
        "import numpy as np\n",
        "import re\n",
        "from re import sub\n",
        "import multiprocessing\n",
        "from unidecode import unidecode\n",
        "import os\n",
        "from time import time \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Flatten, Conv1D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "import h5py\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "UFm90CwhfG6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cricket = pd.read_excel(io=\"Cricket.xlsx\", sheet_name=\"Sheet1\", engine='openpyxl')\n",
        "dataset_cricket.head()"
      ],
      "metadata": {
        "id": "z-CX8s7ufLBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(dataset_cricket['Polarity'])"
      ],
      "metadata": {
        "id": "Gzced3cWfQB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_word_list(text):\n",
        "    text = text.split()\n",
        "    return text\n",
        "\n",
        "def replace_strings(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           u\"\\u00C0-\\u017F\"          #latin\n",
        "                           u\"\\u2000-\\u206F\"          #generalPunctuations\n",
        "                               \n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
        "    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n",
        "    \n",
        "    text=emoji_pattern.sub(r'', text)\n",
        "    text=english_pattern.sub(r'', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_punctuations(my_str):\n",
        "    # define punctuation\n",
        "    punctuations = '''````£|¢|Ñ+-*/=EROero৳০১২৩৪৫৬৭৮৯012–34567•89।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰⚽️✌�￰৷￰'''\n",
        "    \n",
        "    no_punct = \"\"\n",
        "    for char in my_str:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "\n",
        "    # display the unpunctuated string\n",
        "    return no_punct\n",
        "\n",
        "\n",
        "\n",
        "def joining(text):\n",
        "    out=' '.join(text)\n",
        "    return out\n",
        "\n",
        "def preprocessing(text):\n",
        "    out=remove_punctuations(replace_strings(text))\n",
        "    return out"
      ],
      "metadata": {
        "id": "6mC7VmbJfV4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cricket['Text'] = dataset_cricket.Text.apply(lambda x: preprocessing(str(x)))\n",
        "dataset_cricket.reset_index(drop=True, inplace=True)\n",
        "enc = LabelEncoder()\n",
        "dataset_cricket['Polarity'] = enc.fit_transform(dataset_cricket['Polarity'])"
      ],
      "metadata": {
        "id": "Pbgy43fbfkaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1, test1 = train_test_split(dataset_cricket,random_state=69, test_size=0.2)\n",
        "training_sentences_cricket = []\n",
        "testing_sentences_cricket = []\n",
        "\n",
        "\n",
        "\n",
        "train_sentences_cricket=train1['Text'].values\n",
        "train_labels_cricket=train1['Polarity'].values\n",
        "for i in range(train_sentences_cricket.shape[0]): \n",
        "    #print(train_sentences[i])\n",
        "    x=str(train_sentences_cricket[i])\n",
        "    training_sentences_cricket.append(x)\n",
        "    \n",
        "training_sentences_cricket=np.array(training_sentences_cricket)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_sentences_cricket=test1['Text'].values\n",
        "test_labels_cricket=test1['Polarity'].values\n",
        "\n",
        "for i in range(test_sentences_cricket.shape[0]): \n",
        "    x=str(test_sentences_cricket[i])\n",
        "    testing_sentences_cricket.append(x)\n",
        "    \n",
        "testing_sentences_cricket=np.array(testing_sentences_cricket)\n",
        "\n",
        "\n",
        "train_labels_cricket=tf.keras.utils.to_categorical(train_labels_cricket)\n",
        "\n",
        "\n",
        "test_labels_cricket=tf.keras.utils.to_categorical(test_labels_cricket)\n",
        "print(\"Training Set Length: \"+str(len(train1)))\n",
        "print(\"Testing Set Length: \"+str(len(test1)))\n",
        "print(\"training_sentences shape: \"+str(training_sentences_cricket.shape))\n",
        "print(\"testing_sentences shape: \"+str(testing_sentences_cricket.shape))\n",
        "print(\"train_labels shape: \"+str(train_labels_cricket.shape))\n",
        "print(\"test_labels shape: \"+str(test_labels_cricket.shape))"
      ],
      "metadata": {
        "id": "WSmFIgzbfpu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_sentences_cricket[1])\n",
        "print(train_labels_cricket[0])"
      ],
      "metadata": {
        "id": "Hm7gjEG4fxA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(training_sentences_cricket)+1\n",
        "embedding_dim = 100\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "print(training_sentences_cricket.shape)\n",
        "print(train_labels_cricket.shape)"
      ],
      "metadata": {
        "id": "ZeojTFj5f0tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences_cricket)\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "print(\"Word index length:\"+str(len(tokenizer.word_index)))\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences_cricket)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(testing_sentences_cricket)\n",
        "testing_padded = pad_sequences(test_sequences,maxlen=max_length)"
      ],
      "metadata": {
        "id": "woz07Gdgf4wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence :--> \\n\")\n",
        "print(training_sentences_cricket[2]+\"\\n\")\n",
        "print(\"Sentence Tokenized and Converted into Sequence :--> \\n\")\n",
        "print(str(sequences[2])+\"\\n\")\n",
        "print(\"After Padding the Sequence with padding length 100 :--> \\n\")\n",
        "print(padded[2])"
      ],
      "metadata": {
        "id": "peUTD_vLf_KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Padded shape(training): \"+str(padded.shape))\n",
        "print(\"Padded shape(testing): \"+str(testing_padded.shape))"
      ],
      "metadata": {
        "id": "aC7ZYnTIgB7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    pr = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    f1_score = 2 * (pr * rec) / (pr + rec)\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "FXQphroBgFwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_cricket #padded"
      ],
      "metadata": {
        "id": "6WVkVBdTgLyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "\n",
        "detection=Sequential()\n",
        "\n",
        "detection.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "\n",
        "#1 -convolutional layer-1\n",
        "detection.add(Conv1D(64 ,kernel_size=3))\n",
        "detection.add(BatchNormalization())\n",
        "detection.add(Activation('relu'))\n",
        "# detection.add(GlobalMaxPooling1D())\n",
        "detection.add(Dropout(0.1))\n",
        "\n",
        "#2 -convolutional layer-2\n",
        "detection.add(Conv1D(96,kernel_size=3))\n",
        "detection.add(BatchNormalization())\n",
        "detection.add(Activation('relu'))\n",
        "# detection.add(GlobalMaxPooling1D())\n",
        "# detection.add(Dropout(0.2))\n",
        "\n",
        "#2 -convolutional layer-2\n",
        "detection.add(Conv1D(128,kernel_size=3))\n",
        "detection.add(BatchNormalization())\n",
        "detection.add(Activation('relu'))\n",
        "# detection.add(GlobalMaxPooling1D())\n",
        "detection.add(Dropout(0.15))\n",
        "\n",
        "\n",
        "# #2 -convolutional layer-2\n",
        "# detection.add(Conv1D(256,kernel_size=3))\n",
        "# detection.add(BatchNormalization())\n",
        "# detection.add(Activation('relu'))\n",
        "# # detection.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# # detection.add(GlobalMaxPooling1D())\n",
        "# detection.add(Dropout(0.2))\n",
        "\n",
        "# #3 -convolutional layer-3\n",
        "# detection.add(Conv1D(384,kernel_size=3))\n",
        "# detection.add(BatchNormalization())\n",
        "# detection.add(Activation('relu'))\n",
        "# # detection.add(GlobalMaxPooling1D())\n",
        "# # detection.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# # detection.add(UpSampling2D(input_shape=(512, 3, 3)))\n",
        "# detection.add(Dropout(0.2))\n",
        "\n",
        "# #3 -convolutional layer-3\n",
        "# detection.add(Conv1D(512,kernel_size=3))\n",
        "# detection.add(BatchNormalization())\n",
        "# detection.add(Activation('relu'))\n",
        "# # detection.add(GlobalMaxPooling1D())\n",
        "# # detection.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# # detection.add(UpSampling2D(input_shape=(512, 3, 3)))\n",
        "# detection.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "# detection.add(Flatten())\n",
        "# detection.add(Dense(512))\n",
        "# detection.add(BatchNormalization())\n",
        "# detection.add(Activation('relu'))\n",
        "\n",
        "# detection.add(Flatten())\n",
        "# detection.add(Dense(256))\n",
        "# detection.add(BatchNormalization())\n",
        "# detection.add(Activation('relu'))\n",
        "\n",
        "detection.add(Flatten())\n",
        "detection.add(Dense(128))\n",
        "detection.add(BatchNormalization())\n",
        "detection.add(Activation('relu'))\n",
        "\n",
        "detection.add(Flatten())\n",
        "detection.add(Dense(64))\n",
        "detection.add(BatchNormalization())\n",
        "detection.add(Activation('relu'))\n",
        "\n",
        "detection.add(Dense(3,activation='sigmoid'))\n",
        "optimum=Adam(lr=0.00001)\n",
        "detection.summary()\n",
        "detection.compile(optimizer=optimum,loss='binary_crossentropy',metrics=['accuracy', precision, recall])"
      ],
      "metadata": {
        "id": "WV23pd9BgRr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(padded, train_labels_cricket, test_size=0.15, shuffle= True) #data, label"
      ],
      "metadata": {
        "id": "qUyDjLgpgteb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_cricket = detection.fit(x_train, y_train,shuffle=True,epochs=30,batch_size=4,validation_data=(x_valid,y_valid))"
      ],
      "metadata": {
        "id": "ns638bYcg4d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detection.evaluate(x=testing_padded,y=test_labels_cricket,verbose=1)\n",
        "# loss: - accuracy: - recall:  - precision:"
      ],
      "metadata": {
        "id": "RFjD4h0Tg-fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(model_cricket.history['loss'], label='loss_train')\n",
        "plt.plot(model_cricket.history['val_loss'], label='loss_val')\n",
        "# plt.plot(mod.history['accuracy'], label='acc_train')\n",
        "# plt.plot(mod.history['val_accuracy'], label='acc_val')\n",
        "plt.legend()\n",
        "plt.title('Train Val_Loss in Proposed Neural Network')\n",
        "plt.show()\n",
        "plt.savefig('LossVal_Loss')"
      ],
      "metadata": {
        "id": "3dLiryALhEcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(model_cricket.history['accuracy'], label='train acc')\n",
        "plt.plot(model_cricket.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.title('TrainVal_Acc in Proposed Neural Network')\n",
        "plt.show()\n",
        "plt.savefig('AccVal_Acc')"
      ],
      "metadata": {
        "id": "GG2-u8LuhIu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Approach 5\n"
      ],
      "metadata": {
        "id": "Xom7PMlzoQ19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing y\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "le = LabelEncoder()\n",
        "labelEncode = le.fit_transform(data[\"category\"])\n",
        "print(\"LabelEncode\")\n",
        "print(labelEncode)\n",
        "categorical_y = to_categorical(labelEncode)\n",
        "print(\"To_Categorical\")\n",
        "print(categorical_y)"
      ],
      "metadata": {
        "id": "9pUfYkWNpbIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#preparing x for CNN\n",
        "MAX_FEATURES = 20001\n",
        "\n",
        "onehot_corpus = []\n",
        "for text in textList:\n",
        "    onehot_corpus.append(one_hot(text,MAX_FEATURES))\n",
        "    \n",
        "maxTextLen = 0\n",
        "for text in textList:\n",
        "    word_token=word_tokenize(text)\n",
        "    if(maxTextLen < len(word_token)):\n",
        "        maxTextLen = len(word_token)\n",
        "        \n",
        "print(\"Max number of words : \",maxTextLen)\n",
        "\n",
        "padded_corpus=pad_sequences(onehot_corpus,maxlen=maxTextLen,padding='post')\n",
        "x_train2,x_test2,y_train2,y_test2 = train_test_split(padded_corpus,categorical_y,test_size=0.33,random_state=42)"
      ],
      "metadata": {
        "id": "tKOqUajFoTxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\n",
        "def build_cnn_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(MAX_FEATURES, 100, input_length=maxTextLen))\n",
        "\n",
        "\n",
        "    model.add(Conv1D(64, 2, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    #model.add(MaxPooling1D(2))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(units=1024,activation=\"relu\"))\n",
        "    model.add(Dense(units=512,activation=\"relu\"))\n",
        "    \n",
        "    model.add(Dense(units=y_train2.shape[1],activation=\"softmax\"))\n",
        "    \n",
        "    optimizer = Adam(lr=0.000055,beta_1=0.9,beta_2=0.999)\n",
        "    \n",
        "    model.compile(optimizer=optimizer,metrics=[\"accuracy\"],loss=categorical_crossentropy)\n",
        "    return model"
      ],
      "metadata": {
        "id": "n6Rg-0C6oYap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = build_cnn_model()\n",
        "plot_model(cnn_model,show_shapes=True)"
      ],
      "metadata": {
        "id": "a3dt-6XFoiie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_history = cnn_model.fit(x_train2,y_train2,epochs=10,batch_size=1280,shuffle=True)\n",
        "ypred2 = cnn_model.predict(x_test2)"
      ],
      "metadata": {
        "id": "_GNQLXw-onQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_accuracy = accuracy_score(y_test2.argmax(axis=-1),ypred2.argmax(axis=-1))\n",
        "#print(\"CNN Accuracy:\",cnn_accuracy)\n",
        "cnn_cn = confusion_matrix(y_test2.argmax(axis=-1),ypred2.argmax(axis=-1))\n",
        "plt.subplots(figsize=(18,14))\n",
        "sns.heatmap(cnn_cn,annot=True,fmt=\"1d\",cbar=False,xticklabels=le.classes_,yticklabels=le.classes_)\n",
        "plt.title(\"CNN Accuracy: {}\".format(cnn_accuracy),fontsize=50)\n",
        "plt.xlabel(\"Predicted\",fontsize=15)\n",
        "plt.ylabel(\"Actual\",fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4c5UYmX9orxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
        "axe1[0].plot(cnn_history.history[\"accuracy\"],label=\"accuracy\",color=\"blue\")\n",
        "axe1[1].plot(cnn_history.history[\"loss\"],label=\"loss\",color=\"red\")\n",
        "axe1[0].title.set_text(\"CNN Accuracy\")\n",
        "axe1[1].title.set_text(\"CNN Loss\")\n",
        "axe1[0].set_xlabel(\"Epoch\")\n",
        "axe1[1].set_xlabel(\"Epoch\")\n",
        "axe1[0].set_ylabel(\"Rate\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SUmu8vbRoxDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_predict(text):\n",
        "    puretext = leadMyWord(text)\n",
        "    onehottext = one_hot(puretext,MAX_FEATURES)\n",
        "    text_pad = pad_sequences([onehottext],maxlen=maxTextLen,padding='post')\n",
        "    predicted = cnn_model.predict(text_pad)\n",
        "    predicted_category = predicted.argmax(axis=1)\n",
        "    return le.classes_[predicted_category]\n",
        "    \n",
        "for _ in range(10):\n",
        "    randint = np.random.randint(len(data))\n",
        "    text = data.text[randint]  \n",
        "    print(\"  Text\")\n",
        "    print(\"-\"*8)\n",
        "    print(text)\n",
        "    print(\"-\"*20)\n",
        "    print(\"Actual Category: {}\".format(data.category[randint]))\n",
        "    print(\"ANN Predicted Category: {}\".format(ann_predict(text)[0]))\n",
        "    print(\"CNN Predicted Category: {}\".format(cnn_predict(text)[0]))\n",
        "    print(\"*\"*50)"
      ],
      "metadata": {
        "id": "PG7yXah5pDUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let me try it too\n",
        "def predict_print(text):\n",
        "    print(\"  Text\")\n",
        "    print(\"-\"*8)\n",
        "    print(text)\n",
        "    print(\"-\"*20)\n",
        "    print(\"ANN Predicted Category: {}\".format(ann_predict(text)[0]))\n",
        "    print(\"CNN Predicted Category: {}\".format(cnn_predict(text)[0]))\n",
        "    print(\"*\"*50)\n",
        "myText = \"Yemeğin içinden kıl çıktı, gitmenizi önermiyorum.\" # hair came out of the dish, I don't suggest you go\n",
        "predict_print(myText)\n",
        "myText = \"Tuş bozuk.\" # Key Broken\n",
        "predict_print(myText)"
      ],
      "metadata": {
        "id": "WiGNo2xfpFxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lt4hmxZF_ID4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DxKFlu-w_Ht1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "batch_size = 100\n",
        "TensorFlow Session\n",
        "with tf.Session() as sess:\n",
        "    # Initialize all variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Add the model graph to TensorBoard\n",
        "    writer.add_graph(sess.graph)\n",
        "    \n",
        "    # Loop over number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        train_accuracy = 0\n",
        "        \n",
        "        for batch in range(0, int(len(data.train.labels)/batch_size)):\n",
        "            \n",
        "            # Get a batch of images and labels\n",
        "            x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
        "            \n",
        "            # Put the batch into a dict with the proper names for placeholder variables\n",
        "            feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
        "            \n",
        "            # Run the optimizer using this batch of training data.\n",
        "            sess.run(optimizer, feed_dict=feed_dict_train)\n",
        "            \n",
        "            # Calculate the accuracy on the batch of training data\n",
        "            train_accuracy += sess.run(accuracy, feed_dict=feed_dict_train)\n",
        "            \n",
        "            # Generate summary with the current batch of data and write to file\n",
        "            summ = sess.run(merged_summary, feed_dict=feed_dict_train)\n",
        "            writer.add_summary(summ, epoch*int(len(data.train.labels)/batch_size) + batch)\n",
        "        \n",
        "          \n",
        "        train_accuracy /= int(len(data.train.labels)/batch_size)\n",
        "        \n",
        "        # Generate summary and validate the model on the entire validation set\n",
        "        summ, vali_accuracy = sess.run([merged_summary, accuracy], feed_dict={x:data.validation.images, y_true:data.validation.labels})\n",
        "        writer1.add_summary(summ, epoch)\n",
        "        \n",
        "\n",
        "        end_time = time.time()\n",
        "        \n",
        "        print(\"Epoch \"+str(epoch+1)+\" completed : Time usage \"+str(int(end_time-start_time))+\" seconds\")\n",
        "        print(\"\\tAccuracy:\")\n",
        "        print (\"\\t- Training Accuracy:\\t{}\".format(train_accuracy))\n",
        "        print (\"\\t- Validation Accuracy:\\t{}\".format(vali_accuracy))"
      ],
      "metadata": {
        "id": "VzKq3-eY_JQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}